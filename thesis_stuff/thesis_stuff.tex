\documentclass[10pt]{article} % Font size - 10pt, 11pt or 12pt

\usepackage[hmargin=1.25cm, vmargin=1.5cm]{geometry} % Document margins

\usepackage{marvosym} % Required for symbols in the colored box

\usepackage[usenames,dvipsnames]{xcolor} % Allows the definition of hex colors

% Fonts and tweaks for XeLaTeX
\usepackage{fontspec,xltxtra,xunicode}
\defaultfontfeatures{Mapping=tex-text}
%\setmonofont[Scale=MatchLowercase]{Andale Mono}

% Colors for links, text and headings
\usepackage{hyperref}
\definecolor{linkcolor}{HTML}{506266} % Blue-gray color for links
\definecolor{shade}{HTML}{F5DD9D} % Peach color for the contact information box
\definecolor{text1}{HTML}{2b2b2b} % Main document font color, off-black
\definecolor{headings}{HTML}{701112} % Dark red color for headings
% Other color palettes: shade=B9D7D9 and linkcolor=A40000; shade=D4D7FE and linkcolor=FF0080

\hypersetup{colorlinks,breaklinks, urlcolor=linkcolor, linkcolor=linkcolor} % Set up links and colors

\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\pagestyle{fancy}
\fancyhf{}
% Headers and footers can be added with the \lhead{} \rhead{} \lfoot{} \rfoot{} commands
% Example footer:
%\rfoot{\color{headings} {\sffamily Last update: \today}. Typeset with Xe\LaTeX}

\renewcommand{\headrulewidth}{0pt} % Get rid of the default rule in the header

\usepackage{titlesec} % Allows creating custom \section's

% Format of the section titles
\titleformat{\section}{\color{headings}
\scshape\Large\raggedright}{}{0em}{}[\color{black}\titlerule]

\title{Dynein project log}
\author{Elliott Capek}
\titlespacing{\section}{0pt}{0pt}{5pt} % Spacing around titles

\begin{document}

\maketitle{}

\section{Equipartition Theorem}
The Equipartition Theorem (ET) predicts the average energy per degree of freedom of a system with energy which depends on the square of each degree of freedom. Mathematically is is expressed as:

\begin{align*}
  Z &= \int_{-\infty}^{\infty} e^{-\beta Ax^2}dx = \sqrt{\frac{\pi}{\beta A}}\\
  <E> &= - \frac{d\ln Z}{d\beta} = -\left(\sqrt{\frac{\beta A}{\pi}}\right)\left(\frac{\sqrt{\beta A}}{2\sqrt{\pi}}\right)\frac{-\pi}{A\beta^2}\\
  &= \frac{1}{2}k_BT\\
\end{align*}

where Z is the partition function and x a degree of freedom.\\

For our model, each domain feels three separate forces:

\textbf{Brownian forces}: Random forces due to water particle collisions. Gaussian-sampled with a
variance $\sqrt{\frac{\gamma k_BT}{dt}}$. Brownian forces are somehow necessary for the ET energy
to be realized.\\

\textbf{Conformational forces}: Spring forces from the molecule deviating from its lowest-energy
conformation. The energy from these forces is quadratic with respect to angle, and so each free
angle should contribute to the average energy by ET.\\

\textbf{Internal forces}. Tension forces (constraint forces) felt between domains which keep
the protein rigid. These are not quadratic, so it is unclear how they contribute to energy!\\

\subsection{Onebound case}
The conformational forces are:\\
\begin{align*}
  |\vec{F_{ub}}| &\approx \frac{c_m\theta_{um}}{L_s}\\
  |\vec{F_{um}}| &\approx \frac{c_m\theta_{um}}{L_s} + \frac{c_m\theta_{um}}{L_t} + \frac{c_t\theta_{t}}{L_t}\\
  |\vec{F_{t}}| &\approx \frac{c_m\theta_{um}}{L_t} + 2\frac{c_t\theta_t}{L_t} + \frac{c_m\theta_{bm}}{L_t}\\
  |\vec{F_{bm}}| &\approx \frac{c_t\theta_t}{L_t} + \frac{c_m\theta_{bm}}{L_t} + \frac{c_m\theta_{bm}}{L_s} + \frac{c_b\theta_{bb}}{L_s}\\
  |\vec{F_{bb}}| &\approx \frac{c_m\theta_{bm}}{L_s} + \frac{c_b\theta_{bb}}{L_s}\\
\end{align*}

From this we can come up with rough order-of-magnitude estimates of the energy of each domain is:

\begin{align*}
  PE_{ub} &\approx \frac{c_m}{L_s}\theta_{um}^2 + \frac{c_m}{L_t}\theta_{um}^2\\
  PE_{t} &\approx \frac{c_t}{L_t}\theta_{t}^2\\
  PE_{bm} &\approx \frac{c_m}{L_s}\theta_{bm}^2 + \frac{c_m}{L_t}\theta_{bm}^2\\
  PE_{bb} &\approx \frac{c_m}{L_s}\theta_{um}^2\\
\end{align*}

From this we can see that the conformational PE of each domain depends on different combinations of
stalk/tail length and spring constants.

\textbf{When is equipartition behaved?}
It seems like ET should be behaved when we have an even balance of Brownian and conformational forces:

\begin{align*}
  F_{conf} &\approx F_{Brownian}\\
  \frac{c}{L}\theta &\approx \sqrt{\frac{k_BT\gamma}{dt}}\\
\end{align*}

Is this reflected in our simulation? Well take a look at %% Fig ($\ref{fig:latent-heat}$)
...

%% \begin{figure}[h!]
%%   \centering
%%   \includegraphics[width=0.5\textwidth]{../figures/ob_equipartition_vs_force_ratio.pdf}
%%   \caption{Onebound conformational energy follows equipartition best when the ratio of conformational
%%   forces to Brownian forces is one.}
%%   \label{fig:equipartition_vs_force_ratio}
%% \end{figure}

\section{Brownian motion}
\subsection{Derivation} \label{sec:bmotion-derivation}
We can derive the equation for the position of a particle using the simple Newtonian mechanics. We
start with the general $2^{nd}$ law equation for a particle with random forces and drag (the
Langevin equation). We then take the time-averages of all variables (using expectation value signs)
to eliminate the effect of random Brownian forces. After some algebra we arrive at a differential
equation for squared position. We solve it and simplify its expression for limits when $t >> \tau$
and $t << \tau$.\\

\begin{align*}
  \frac{d\vec{v}}{dt} &= \frac{1}{m}\left(\frac{-mv}{\tau} + R\right)\\
  \frac{d\vec{v}}{dt} &= \frac{-v}{\tau} + \frac{R}{m}\\
  <\vec{r} \cdot \frac{d\vec{v}}{dt} > &+ \frac{1}{\tau}<\vec{r} \cdot \vec{v}> = 0\\
  \frac{1}{2}\frac{d^2}{dt^2}(r^2) &+ \frac{1}{2\tau}\frac{d}{dt}(r^2) - v^2 = 0 \hspace{1cm} \Bigg(\mbox{b/c }\frac{d}{dt}\left(\vec{r} \cdot \vec{v}\right) = v^2 + \left(\vec{r} \cdot \frac{d\vec{v}}{dt}\right)
  \mbox{ and }
  \frac{d}{dt}\left(\vec{r} \cdot \vec{r}\right) = 2\left(\vec{r} \cdot \vec{v}\right) \Bigg)\\
  \frac{d^2}{dt^2}(<r^2>) + &\frac{1}{\tau}\frac{d}{dt}(<r^2>) = 2<v^2> = \frac{6k_BT}{m} \hspace{1cm}\left(\mbox{Equipartition Theorem}\right)\\
  <r^2> &= \frac{6k_BT\tau^2}{m}\left(e^{-t/\tau}-1+\frac{t}{\tau}\right)\\
\end{align*}

so...

\begin{align*}
  \left(t << \tau \right) \rightarrow <r^2> = \frac{3k_BT}{m}t^2 \hspace{2cm}\mbox{free particle motion}\\
  \left(t >> \tau \right) \rightarrow <r^2> = \frac{6k_BT\tau}{m}t \hspace{2cm}\mbox{motion proportional to }\sqrt{t}\\
\end{align*}

\textbf{Averaging over time?}
In the above derivation we use time averaging to eliminate the effect of Brownian forces. This is
essentially just discussing the motion of a particle on a timescale where Brownian forces go to
zero. The variance of the Brownian force R is:

\begin{align*}
  \sigma_R^2 &= \sqrt{\frac{2k_BTm}{\tau dt}}\\
\end{align*}

(TODO: further justify the dt in the variance)\\
Thus over larger timescales (bigger dt's) the Brownian force is smaller. When time averaging, all
we are saying is that our dt is large enough that Brownian forces are negligible.\\

An interesting observation is that the motion equation for a particle feeling Brownian forces is
identical to the equation for a macroscopic object feeling drag forces, which makes sense, since
to solve the Brownian motion equation we just eliminate R.\\

\section{Brownian dynamics}
We use Brownian dynamics as our equation of motion. BD is just the special case of Langevin dynamics

\begin{align*}
  m\frac{d\vec{v}}{dt} &= -\frac{m}{\tau}\vec{v} + \vec{F} + \vec{R}\\
\end{align*}

where $m\rightarrow0$:

\begin{align*}
  \vec{v} &= \frac{\tau}{m}\vec{F} + \frac{\tau}{m}\vec{R}\\
\end{align*}

where m is the mass, $\tau$ the time constant, $\vec{F}$ a generic force and $\vec{R}$ the Brownian
force. Mass ``going to zero'' means we are discussing particles where $m << $ any other variable.
Because m is tiny the particle effectively has no net force on it, so the drag force must cancel
out the other forces. From this we can find velocity.\\

\subsection{What is $\tau$?}
Tau is a time constant that describes the behavior of a system with respect to time. By looking at
the motion equations for Brownian systems in Section (\ref{sec:bmotion-derivation}), we see that the
size of t with respect to $\tau$ decides what type of motion the particle will feel. On the ``small''
timescale the system displays free particle motion, but on the ``large'' timescale, the system
displays Brownian motion. $\tau$ can be thought of as the time the system switches from free to
Brownian motion.\\

\section{Finding a reasonable dt}
TODO: describe the relationship between diffusion constant D, time constant $\tau$ and dt.\\

\subsection{Relating D to $\tau$}

I don't really understand this derivation:

\begin{align*}
  \sin\left(\Delta\theta\right) &\approx \Delta\theta = \frac{x}{L}\\
  \Delta\theta^2 &= \frac{x^2}{L^2} = \frac{Dt}{L^2}\\
\end{align*}

We set our $\tau$ to be the time when $<\Delta\theta^2> \approx 1$, so:

\begin{align*}
  \tau = \frac{L^2}{D}\\
\end{align*}

\subsection{Finding D}
For a particle feeling Brownian forces:

\begin{align*}
  \frac{\Delta x}{\Delta t} &= \frac{\tau}{m} R\\
\end{align*}

We say the magnitude of R is the standard deviation of a Gaussian process:

\begin{align*}
  \frac{\Delta x}{\Delta t} &= \frac{\tau}{m}\sqrt{\frac{2k_BTm}{\tau \Delta t}}\\
  \Delta x &= \sqrt{\frac{2k_BT\tau}{m\Delta t}}\Delta t\\
  <\Delta x^2> &= \frac{2k_BT\tau}{m}\Delta t\\
\end{align*}

We then use the Brownian motion equation for $t >> \tau$ to find D:\\

\begin{align*}
  <\Delta x^2> &= D\Delta t = \frac{2k_BT\tau}{m}\Delta t\\
  D = \frac{2k_BT\tau}{m} = \frac{2k_BT}{\gamma}\\
\end{align*}

\subsection{Alternative derivation of D (Einstein relation)}
We can also arrive at an expression for D through an equilibrium argument. There are two forces which cause currents in solutions: drift and diffusion.
Drift is motion due to a force acting on particles. Diffusion is current due to random, natural motions of particles. Our equilibrium condition is when
the flux of particles due to both effects is equal, so there is no net particle flow. We first need to describe these fluxes:\\

\textbf{Drift flux}\\
Our particles obey Brownian dynamics, so $\gamma\dot{x} = F$, where $\gamma$ is the drag force constant. Flux J, or particles per second-area, can be
found by multiplying the velocity of particles by the concentration $\rho$:

\begin{align*}
  J_{drift} &= \frac{1}{\gamma}F(x)\rho(x)\\
  &= -\frac{1}{\gamma}\rho(x)\nabla U\\
\end{align*}

\textbf{Diffusion flux}\\
Flux due to diffusion is proportional to the negative gradient in concentration and D:

\begin{align*}
  J_{diff} &= -D\nabla \rho(x)\\
\end{align*}

\textbf{Solving for D}\\
The concentration gradient in a solution is proportional to the energy gradient. By the chain rule:

\begin{align*}
  \nabla \rho &= \frac{\partial\rho}{\partial U} \nabla U\\
\end{align*}

We then set the sum of our two fluxes to zero and solve for the equilibrium condition:

\begin{align*}
  J_{diff} + J_{drift} &= 0\\
  -\frac{1}{\gamma}\rho(x)\nabla U - D\nabla \rho(x) &= 0\\
  \frac{1}{\gamma}\rho(x)\nabla U + D \frac{\partial\rho}{\partial U} \nabla U&= 0\\
  \left(\frac{1}{\gamma}\rho(x) + D\frac{\partial\rho}{\partial U}\right)\nabla U &= 0\\
\end{align*}

because $\nabla U$ is not guarenteed to be zero everywhere, it must be the case that:

\begin{align*}
  \frac{1}{\gamma}\rho(x) &= -D\frac{\partial\rho}{\partial U}\\
  \frac{-\rho(x)}{\gamma\frac{\partial\rho}{\partial U}} &= D\\
\end{align*}

\textbf{Finding d$\rho$/dU}\\
To simplify this value, we note that, because the particles do not interact, energy is purely a function of position. Thus, by
Maxwell-Boltzman statistics, the probability of being at a location x (i.e $\rho(x)$) is proportional to $e^{-\beta U(x)}$.
From this we can find an expression for concentration and d$\rho(x)/dU(x)$:

\begin{align*}
  \rho(x) &= Ae^{-\beta U(x)}\\
  \frac{\partial \rho(x)}{\partial U} &= \frac{-1}{k_BT}Ae^{-\beta U(x)}\\
\end{align*}

Plugging these expressions into our D equation, in addition to our definition for $\gamma$, we can see that:

\begin{align*}
  \frac{-1}{\gamma\frac{-1}{k_BT}} &= D\\
  D &= \frac{k_BT}{\gamma} = \frac{k_BT\tau}{m}\\
\end{align*}

\section{Equipartition Theorem for quadratic energies}
\begin{align*}
  Z &= \int_{-\infty}^{\infty} e^{-\beta Ax^2}dx = \sqrt{\frac{\pi}{\beta A}}\\
  <E> &= - \frac{d\ln Z}{d\beta} = -\left(\sqrt{\frac{\beta A}{\pi}}\right)\left(\frac{\sqrt{\beta A}}{2\sqrt{\pi}}\right)\frac{-\pi}{A\beta^2}\\
  &= \frac{k_BT}{2}\\
\end{align*}

\section{Gamma derivation}

Stoke's Law states that the relationship between viscosity, force, and
velocity for a spherical particle is given by:
\begin{align}
  \mathbf{v} &= \frac{\mathbf{F}}{6\pi \mu R}
\end{align}
where $\mu$ is the viscosity of the fluid, and $R$ is the radius of
the particle.  Thus in our terminology:
\begin{align}
  \gamma &= 6\pi \mu R.
\end{align}
Note that the viscosity of water is temperature-dependent and around
0.7~mPa~s at body temperature.  However, the viscosity of cytoplasm
may be rather different, and will of course depend on length scale.
But for our purposes we can probably get away with this value.

The random force $\mathbf{R}(t)$ should have a Gaussian distribution
with variance~1, but then be divided by $\sqrt{\Delta t}$.  Thus, the
smaller the time step, the larger the random force we will apply.  We
can understand this in the backwards direction.  When we observe a
larger time step, the ``random force'' results from many smaller
random forces added together, and these smaller forces often cancel,
leading to this reduction in the total force over that time period.

Finally, we will need estimates for the radii of each domain (to go
into the $\gamma$ parameter, the equilibrium angles for each hinge, and
the spring constants for each hinge.  The first two we can estimate
from the structure of dynein.  The third will require a rough guess.
We can get a ball-park guess by deciding how much we want the angle to
fluctuate (e.g. $\ll \pi$ for each angle!), and then take our spring
constant to be $kT$ divided by that anglular fluctuation squared.

\section{Background of Correlation function}
This is based on the Autocorrelation procedure (see Convolution on Wikipedia).
In our correlation function code, we take the autocorrelation function of our
potential and time-translate it to get the time at which there is zero correlation
between the functions. Flesh this out sometime!\\

\section{CLT (and/or Brownian distribution is Gaussian?) Derivation}
Here we show that, given an initial PDF as a delta function $P(t_0,x) = \delta(x)$
and any transition likelihood $W(s)$ for some step size s and 1 timestep, P
eventually evolves into a Gaussian function.\\

We use a discrete time, continuous position model:\\

\begin{align}
  P(t_0+1,x') &= \int_{-\infty}^{\infty}W(x'-x)P(t_0,x)dx
\end{align}

This matches the form of a convolution integral:

\begin{align*}
  (P*W)(x') &= \int_{-\infty}^{\infty}P(t_0,x)W(x'-x)dx
\end{align*}

By the Convolution Theorem (prove?) we can simplify the above expression:

\begin{align}
  F\{P(t_0+1,x')\} &= F\left\{\int_{-\infty}^{\infty}W(x'-x)P(t_0,x)dx\right\}\\
  \widetilde{P}(t_0+1,k) &= \widetilde{W}(k)\widetilde{P}(t_0,k)
\end{align}

We now let our initial time $t_0$ be zero and note that, because
$W_n(\Delta x) = W(\Delta x)^n$, we can simplify our expression to:

\begin{align}
  \widetilde{P}(n,k) &= \widetilde{W}(k)^n\widetilde{P}(t_0,k)
\end{align}

We then take the Inverse Fourier Transform of this expression, yielding:

\begin{align}
  P(n,x) &= \int_{-\infty}^{\infty}\widetilde{P}(n,k)e^{2\pi ikx}dk
  = \int_{-\infty}^{\infty}\widetilde{W}(k)^n\widetilde{P}(t_0,k)e^{2\pi ikx}dk
\end{align}

Since $P(0,x)$ is a delta function, we know that $\widetilde{P}(0,x) = 1$, so:

\begin{align}
  P(n,x) &=  \int_{-\infty}^{\infty}\widetilde{W}(k)^ne^{2\pi ikx}dk
\end{align}

The Taylor Series for $\widetilde{W}(k)$ is:

\begin{align*}
  \widetilde{W}(k) &= \widetilde{W}(0) + k\widetilde{W_1}(0) + \frac{k^2}{2}\widetilde{W_2}(0) + \frac{k^3}{6}\widetilde{W_3}(0) + ...\\
  &= 1 + k\widetilde{W_1}(0) + \frac{k^2}{2}\widetilde{W_2}(0) + \frac{k^3}{6}\widetilde{W_3}(0) ...\\
\end{align*}

We define a variable $\widetilde{W}$ such that:

\begin{align*}
  \widetilde{W}(k) &= 1 - \widetilde{C}\\
  \widetilde{C} &= -(k\widetilde{W_1}(0) + \frac{k^2}{2}\widetilde{W_2}(0) + \frac{k^3}{6}\widetilde{W_3}(0) ...)\\
\end{align*}

This allows us to substitute in $\widetilde{C}$ and use the Binomial Expansion:

\begin{align}
  P(n,x) &=  \int_{-\infty}^{\infty}\left(1 - \widetilde{C}\right)^ne^{2\pi ikx}dk\\
  &= \int_{-\infty}^{\infty}\left(\sum_{\alpha=0}^n {}_nC_\alpha\left(-\widetilde{C}\right)^\alpha\right)e^{2\pi ikx}dk\\
  &= \int_{-\infty}^{\infty}\left(\sum_{\alpha=0}^n \frac{n!(-1)^\alpha}{(n-\alpha)!\alpha!} \widetilde{C}^\alpha\right)e^{2\pi ikx}dk
\end{align}

The Taylor Series of our goal, the Gaussian function, is:\\

\begin{align*}
  e^{\frac{-x^2}{2\sigma^2}} &= \sum_{\alpha=0}^\infty \frac{(-1)^\alpha}{\alpha!}\left(\frac{x}{\sqrt{2}\sigma}\right)^{2\alpha}
\end{align*}

We want to make our expression for $P(n,x)$ resemble this. We begin by saying that, for sufficiently large $n$, the higher-order
$\alpha$ terms in the sum go to zero due to the $1/\alpha!$. Thus, we can extend our sum to infinity:\\

\begin{align}
  P(n,x) &= \int_{-\infty}^{\infty}\left(\sum_{\alpha=0}^\infty \frac{(-1)^\alpha}{\alpha!}\frac{n!}{(n-\alpha)!} \widetilde{C}^\alpha\right)e^{2\pi ikx}dk
\end{align}

We then apply Sterling's Approximation in the form $Z! = e^{Z\ln(Z)-Z}$:

\begin{align}
  P(n,x) &= \int_{-\infty}^{\infty}\left(\sum_{\alpha=0}^\infty \frac{(-1)^\alpha}{\alpha!}\frac{e^{n\ln(n)-n}}{e^{(n-\alpha)\ln(n-\alpha)-(n-\alpha)}} \widetilde{C}^\alpha\right)e^{2\pi ikx}dk\\
  &= \int_{-\infty}^{\infty}\left(\sum_{\alpha=0}^\infty \frac{(-1)^\alpha}{\alpha!}\frac{n^ne^{-n}}{(n-\alpha)^{n-\alpha}e^{\alpha-n}} \widetilde{C}^\alpha\right)e^{2\pi ikx}dk
\end{align}

Because n is large, we can get rid of its decaying exponentials and simplify:\\

\begin{align}
  P(n,x) &= \int_{-\infty}^{\infty}\left(\sum_{\alpha=0}^\infty \frac{(-1)^\alpha}{\alpha!}\frac{n^n}{(n-\alpha)^{n-\alpha}e^{\alpha}} \widetilde{C}^\alpha\right)e^{2\pi ikx}dk\\
  &= \int_{-\infty}^{\infty}\left(\sum_{\alpha=0}^\infty \frac{(-1)^\alpha}{\alpha!}\frac{n^n}{(n-\alpha)^{n-\alpha}} \sqrt{\frac{\widetilde{C}}{e}}^{2\alpha}\right)e^{2\pi ikx}dk\\
  &= \int_{-\infty}^{\infty}\left(\sum_{\alpha=0}^\infty \frac{(-1)^\alpha}{\alpha!}\frac{n^\alpha}{(1-\frac{\alpha}{n})^{n-\alpha}} \sqrt{\frac{\widetilde{C}}{e}}^{2\alpha}\right)e^{2\pi ikx}dk
\end{align}

%% We now use the following simplification, assuming that $\alpha/n < 1$:

%% \begin{align*}
%%   \left(1-\frac{\alpha}{n}\right)^{n-\alpha} &= \sum_{x=0}^{n-\alpha} \frac{(n-\alpha)!}{(n-\alpha-x)!x!} \left(\frac{\alpha}{n}\right)^x\\
%%   &= 1 - (n-\alpha)\left(\frac{\alpha}{n}\right) + (n-\alpha)(n-\alpha-1)\left(\frac{\alpha}{n}\right)^2\\
%%   &= 1 - \alpha + \frac{\alpha^2}{n} + \alpha^2 + \frac{-\alpha^3}{n} + \frac{-\alpha^2}{n} + \frac{-\alpha^3}{n} + \frac{\alpha^4}{n^2} + \frac{\alpha^3}{n^2}\\
%%   &= 1 - \alpha + \alpha^2 + \frac{-2\alpha^3}{n} + \frac{\alpha^4}{n^2} + \frac{\alpha^3}{n^2}\\
%% \end{align*}

We then supply the middle factorial fraction with a name, $m = \frac{1}{(1-\frac{\alpha}{n})^{n-\alpha}}$:

\begin{align}
  P(n,x) &= \int_{-\infty}^{\infty}\left(\sum_{\alpha=0}^\infty \frac{m(-1)^\alpha}{\alpha!}\sqrt{\frac{n\widetilde{C}}{e}}^{2\alpha}\right)e^{2\pi ikx}dk
\end{align}

\textbf{TODO: figure out why m is there}

We then are \textit{almost} in a form where we can invert the Gaussian Taylor Expansion. We try it anyway, ignoring m. We also make the assumptions
that our second $\widetilde{W}_1(0)$ Taylor Expansion term in $\widetilde{C}$ is zero and the third term is negative.\\

\begin{align}
  P(n,x) &= \int_{-\infty}^{\infty}e^{\frac{-k^2}{\sqrt{2}\frac{e}{|nW_2(0)|}}}e^{2\pi ikx}dk\\
  &= \int_{-\infty}^{\infty}e^{\frac{-k^2}{\sqrt{2}\sigma^2}}e^{2\pi ikx}dk
\end{align}

We finally use the fact that Gaussians take the same form in real and Fourier space to undo the transform:

\begin{align}
  P(n,x) &= e^{\frac{-k^2}{\sqrt{2}\sigma^2}}\\
\end{align}

where

\begin{align}
  \sigma^2 = \frac{e}{n|W_2(0)|}\\
\end{align}

%% We then solve for $\widetilde{W}_2(0)$ to find our variance:

%% \begin{align}
%%   \widetilde{W}_2(0) &= \frac{d^2}{dk^2}\widetilde{W}(0)\\
%%   &= \left(\frac{d^2}{dk^2}\int_{-\infty}^{\infty}W(x)e^{2\pi ikx}dx\right)_{k=0}\\
%% \end{align}

\section{Alternative CLT Derivation}
Here we show that, given an initial PDF as a delta function $P(t_0,x) = \delta(x)$
and any transition likelihood $W(s)$ for some step size s and 1 timestep, P
eventually evolves into a Gaussian function.\\

We use a discrete time, continuous position model:\\

\begin{align}
  P(t_0+1,x') &= \int_{-\infty}^{\infty}W(x'-x)P(t_0,x)dx
\end{align}

This matches the form of a convolution integral:

\begin{align*}
  (P*W)(x') &= \int_{-\infty}^{\infty}P(t_0,x)W(x'-x)dx
\end{align*}

By the Convolution Theorem (prove?) we can simplify the above expression:

\begin{align}
  F\{P(t_0+1,x')\} &= F\left\{\int_{-\infty}^{\infty}W(x'-x)P(t_0,x)dx\right\}\\
  \widetilde{P}(t_0+1,k) &= \widetilde{W}(k)\widetilde{P}(t_0,k)
\end{align}

We see that in general, the transformed probability distribution after n
timsteps is given by:\\

\begin{align}
  \widetilde{P}(t_0+n,k) &= \widetilde{W}(k)\widetilde{P}(t_0+n-1,k)
\end{align}

Thus our distribution after n timesteps can be expressed as:

\begin{align}
  \widetilde{P}(t_0+n,k) &= \widetilde{W}^n(k)\widetilde{P}(t_0,k)
\end{align}

We wish to solve for the non-transformed probability distribution, so we
apply the inverse Fourier Transform:

\begin{align}
  P(t_0+n,k) &= \frac{1}{2\pi}\int_{-\infty}^{\infty}\widetilde{W}^n(k)\widetilde{P}(t_0,k)e^{ikx}dk
\end{align}

We specified our initial probability as a delta function, which Fourier Transforms to unity:

\begin{align}
  P(t_0+n,k) &= \frac{1}{2\pi}\int_{-\infty}^{\infty}\widetilde{W}^n(k)e^{ikx}dk
\end{align}

We now introduce a new definition $\widetilde{W} = e^{\widetilde{L}}$:\\

\begin{align}
  P(t_0+n,k) &= \frac{1}{2\pi}\int_{-\infty}^{\infty}e^{n\widetilde{L}}e^{ikx}dk
\end{align}

We now need to expand $\widetilde{L}$. We use a Taylor Series:

\begin{align*}
  \widetilde{L}(k) &= \ln(\widetilde{W}) = \sum\frac{k^n\frac{d^n}{dk^n}\ln(\widetilde{W})}{n!}\\
  &= \ln\left(\widetilde{W}(0)\right) + k\frac{\widetilde{W}_1(0)}{\widetilde{W}(0)}
  + \frac{k^2}{2}\left(\frac{\widetilde{W}_2(0)}{\widetilde{W}(0)} - \frac{\widetilde{W}_1(0)^2}{\widetilde{W}_0(0)^2}\right)
  + \frac{k^3}{6}\left(\frac{2\widetilde{W}_1(0)^3}{\widetilde{W}_(0)^3} + \frac{\widetilde{W}_3(0)}{\widetilde{W}(0)}
     - \frac{3\widetilde{W}_1(0)\widetilde{W}_2(0)}{\widetilde{W}(0)^2}\right)\\
  &+ \frac{k^4}{24}\left(\frac{6\widetilde{W}_1(0)^2\widetilde{W}_2(0)}{\widetilde{W}(0)^3} - \frac{6\widetilde{W}_1(0)^4}{\widetilde{W}(0)^4}
  - \frac{3\widetilde{W}_2(0)^2}{\widetilde{W}(0)^2} - \frac{3\widetilde{W}_1(0)\widetilde{W}_3(0)}{\widetilde{W}(0)^2}
  + \frac{6\widetilde{W}_1(0)^2\widetilde{W}_2(0)}{\widetilde{W}(0)^3} + \frac{\widetilde{W}_4(0)}{\widetilde{W}(0)}
  - \frac{\widetilde{W}_1(0)\widetilde{W}_3(0)}{\widetilde{W}(0)^2}\right) + O(5)
\end{align*}

We then make the following assumption: \textbf{Assume all odd terms in $\widetilde{W}$'s expansion are zero}. We justify this by imposing that
$\widetilde{W}$ must be symmetric about the y-axis. We also say that $\widetilde{W}_1(0)=1$, which I don't know how to justify -- (just because
W is normalized to one doesn't mean its maximum point equals one?). These assumptions allow us to simplify $\widetilde{L}$:\\

\begin{align*}
  \widetilde{L}(k) &= \frac{k^2}{2}\widetilde{W}_2(0) + \frac{k^4}{24}\left(\widetilde{W}_4(0) - 3\widetilde{W}_2(0)\right)
\end{align*}

I'm not really sure how to justify this part either:

\begin{align}
  \widetilde{L}(k) &\approx \frac{k^2}{2}\widetilde{W}_2(0)
\end{align}

We then plug this into our definition for $P(t_0+n,k)$:

\begin{align}
  P(t_0+n,k) &= \frac{1}{2\pi}\int_{-\infty}^{\infty}e^{n\frac{k^2}{2}\widetilde{W}_2(0)}e^{ikx}dk
\end{align}

In order to be normalizable, all derivatives of $\widetilde{W}$ must be negative. Thus we rewrite
$\widetilde{W}_2$:

\begin{align}
  P(t_0+n,k) &= \frac{1}{2\pi}\int_{-\infty}^{\infty}e^{-n\frac{k^2}{2}|\widetilde{W}_2(0)|}e^{ikx}dk
\end{align}

This takes the form of a Gaussian. We know that Gaussians are invariant under Fourier Transform,
so:\\

\begin{align}
  P(t_0+n,k) &= e^{\frac{k^2}{2\sigma^2}}
\end{align}

where $\sigma = \sqrt{\frac{1}{n\widetilde{W}_2(0)}}$. As expected by the CLT, the standard deviation of our
new probability depends on $\frac{1}{\sqrt{n}}$!\\

We now try and find more out about $|\widetilde{W}_2(k)|$:

\begin{align*}
  \widetilde{W}_2(k) &= \frac{d^2}{dk^2}\widetilde{W}(k)\\
  &= \frac{d^2}{dk^2}\int_{-\infty}^\infty W(x)e^{-ikx}dx\\
  &= \frac{d}{dk}\int_{-\infty}^\infty -ixW(x)e^{-ikx}dx\\
  &= -\int_{-\infty}^\infty x^2W(x)e^{-ikx}dx\\
\end{align*}

We are specifically interested in the case where $k=0$:

\begin{align*}
  \widetilde{W}_2(0) &= -\int_{-\infty}^\infty x^2W(x)dx
\end{align*}

We explicitly denote $\widetilde{W}_2$'s negativity:

\begin{align*}
  |\widetilde{W}_2(0)| &= \int_{-\infty}^\infty x^2W(x)dx
\end{align*}

and finally note that, since W is the transition probability per timestep, this looks like the expected squared deviation in a single timestep:

\begin{align*}
  |\widetilde{W}_2(0)| &= \int_{-\infty}^\infty x^2W(x)dx = <\Delta x>
\end{align*}

We plug this back into our Gaussian to get our final answer:

\begin{align}
  P(t_0+n,k) &= e^{\frac{k^2}{2\frac{1}{n<\Delta x>}}}
\end{align}

A $\sigma = \sqrt{\frac{1}{n<W>}}$ does not make sense. For one, $\sqrt{<\Delta x>}$ should be in the numerator, since it is the
single-timestep standard deviation. Plus it seems like for longer times the distribution would get more spread out, not less, hence
$\sqrt{n}$ should be in the numerator as well? But the CLT says it should be in the denominator...\\
\\

\section{Un/binding constants}
We let our un/binding rates $k_b$ and $k_u$ be, according to the Arrhenius equation:

\begin{align}
  k_b &= A_be^{\Delta G_{u\rightarrow b}\beta}\\
  k_u &= A_ue^{\Delta G_{b\rightarrow u}\beta}
\end{align}

Our Gibb's free energy changes are given as functions of angular state $\theta$:

\begin{align}
  \Delta G_{u\rightarrow b}(\theta) &= \sum_i \frac{1}{2}c_{bi}\Delta\theta_{bi}^2 - \sum_i \frac{1}{2}c_{ui}\Delta\theta_{ui}^2 + G_{bind}\\
  \Delta G_{u\rightarrow b}(\theta) &= \sum_i \frac{1}{2}c_{ui}\Delta\theta_{ui}^2 - \sum_i \frac{1}{2}c_{bi}\Delta\theta_{bi}^2 + G_{unbind}\\
\end{align}

\section{Papers}
\subsection{Winch Model (dx.doi.org/10.1016/j.bpj.2014.06.022)}
-Communication between motors and BDs possible through $\alpha$-helix sliding. Bidirectional communication. [24,25]\\
-Pre, post-powerstroke states\\
-Pre-powerstroke: unbound state after ATP hydrolysis (our onebound)\\
-Post-powerstroke: bound state (our bothbound)\\
-Elastic stalk, unlike our rigid one\\
-Only tail angle changes between pre and post-powerstroke\\
-MTBD decides directionality?\\
-Tail-tail interaction, also motor-motor stacking interaction\\
-rate constant simulation, does not do dynamics to take into account time spent in a state\\

\textbf{Physical Parameters from Winch Model paper}:\\
Sarlah et al. use the following physical parameters for Dynein: $L_s = 12$nm, $L_t = 14$nm, binding angle: 34 degrees from vertical, motor
domain radius: 7nm, motor domain angle from vertical: 30 degrees, tail angle: 0 degrees between linkers when poststroke, 40 degrees when prestroke.

\subsection{Annual Review, 8-state Mechanochemical diagram}
Cianfrocco, M*., DeSantis, M*., Leschziner, A., Reck-Peterson, S. ``Mechanism and Regulation of Cytoplasmic Dynein'' \textit{Annu. Rev. Cell. Dev. Biol.} 2015: 31:83-108.\\

Goes over in detail the various steps of the Dynein chemical cycle. Eight-state model as follows:\\

\begin{enumerate}
\item Dynein begins cycle with AAA1 binding site open.
\item ATP then binds to the AAA1 binding site.
\item ATP binding causes ``ring closure,'' where the AAA1 and AAA2 subdomains shift closer together.
\item Ring closure (ie ATP binding) causes the buttress to shift, causing the MTBD to lose MT affinity.
\item Ring closure (ATP binding) also causes the linker to bend 90 degrees, causing the monomer to enter the pre-powerstroke state. The order of states 4. and 5. is unknown, but entering pre-powerstroke \textit{after} unbinding from the MT causes more directional motion. The role of ATP hydrolysis also is not understood as of this paper (2015).
\item Random walk of the low-MT-affinity MTBD eventually leads to weak rebinding at a new place on the MT.
\item Rebinding causes a change in the MTBD back to its high affinity state. AAA4 moves to prevent steric clash when the linker straightens. AAA5/6 move away from AAA1 and the linker straightens to its post-powerstroke conformation. NOTE: this is distinct from the conformation in steps 1-4 in that AAA1/2 is still closed.
\item The linker shifts some more and AAA1/2 opens up, releasing ADP.
\end{enumerate}

\subsection{Weihong's Dynein stepping data}
Qiu, W., Derr, N., Goodman, B., Villa, E., Wu, D., Shih, W., Reck-Pterson, S. ``Dynein achieves processive motion using both stochastic and coordinated stepping.'' \textit{Nature Structural \& Molecular Biology}. 2012, 19:193-200.\\

Has the stepping histograms we are trying to reproduce computationally.

\subsection{Pre-powerstroke structure, linker remodel details}
Schmidt, H*., Zalyte, R*., Urnavicius, L., Carter, AP. ``Structure of human cytoplasmic dynein-2 primed for its power stroke.'' \textit{Nature} 2014. 518:435-438.

Describes how ATP hydrolysis ``closes the ring,'' sterically interacting with the linker to cause the pre-powerstroke conformation. Does so by presenting the crystal structure of Dynein AAA1-ADP.Vanadate. Goes over AAs responsible for nucleotide binding. Supplemental video of AAA1L-AAA2L gap closing in response to nucleotide-Vi binding. Supplemental video of linker swinging in response to ring closure. AAA2-4 system acts rigidly during the cycle. Discusses how steric clash is what makes the linker take on a 90-degree bent shape when the ring closes. Figure on Dynein-ADP buttress having no kink, whereas Dynein-ADP.Vi having a kink. The kink presumably changes MTDB binding affinity.

\end{document}
